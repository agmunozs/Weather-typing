{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94373744-6b06-4cfa-b6e1-231d2787fb3e",
   "metadata": {},
   "source": [
    "# PyWR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62805c1-d326-459f-a9c6-ab2914f0501c",
   "metadata": {},
   "source": [
    "Loading required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decdb68a-506e-4392-b067-ad140272feb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import eccodes\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy import feature\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy import interpolate\n",
    "import string\n",
    "# key PyWR functions are imported here\n",
    "from PyWR import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e333e001-a518-4f80-b162-4a72342205e5",
   "metadata": {},
   "source": [
    "Defining constants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3c1423-f86e-4361-a034-f639b011ea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial domain\n",
    "nla=50 \t# Northernmost latitude\n",
    "sla=20 \t# Southernmost latitude\n",
    "wlo=-130 \t# Westernmost longitude\n",
    "elo=-65 \t# Easternmost longitude\n",
    "#Time domain:\n",
    "season='Jan-Dec'\n",
    "yeari=1982\n",
    "yeare=2021\n",
    "#Indicate if force download all data (True in case it's corrupted or new one is needed)\n",
    "force=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff244da-00f8-4add-854e-38ea85f98de0",
   "metadata": {},
   "source": [
    "# Prepare and Download Data (if needed)\n",
    "\n",
    "This section downloads data (as needed; details below).\n",
    "\n",
    "Important: some of the datasets are huge. If the data is already downloaded and ok (e.g., not corrupted) but we wanted to re-download it, set `force_download=True` (each dataset can be controlled independently below as an input to do download_data() function, or simultaneuously by defining the variable above).\n",
    "\n",
    "First, let's create a `data` and a `figs` folder. If already there, it won't create anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68394b93-6403-4f02-8d90-b86b2c79ad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p DWR\n",
    "!cd DWR\n",
    "!mkdir -p DWR/data\n",
    "!mkdir -p DWR/figs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957b5827-37f4-49a8-a817-769cd04ea263",
   "metadata": {},
   "source": [
    "To download data from the IRI data library, you need an authentication key. This is stored in a file called `.IRIDLAUTH`, but is not part of the GitHub repository -- you need to contact the IRI Data Library to request access. Once you have done so, you can put your own authentication key in a file called `.IRIDLAUTH` and use this code. This is a moderately annoying step, and we apologize, but it is required by the S2S Database Terms and Conditions and is necessary for us to share all our code while maintaining some security.\n",
    "\n",
    "**NB** if you're using `git`, be sure to add `.IRIDLAUTH` to your `gitignore` file :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e202ddff-da25-4359-bd5d-051d478478e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/.IRIDLAUTH') as file:\n",
    "   authkey = file.read() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294c82a4-4fd2-4783-9349-746073429c87",
   "metadata": {},
   "source": [
    "# Reanalysis and Observation data\n",
    "\n",
    "Here we download all the data needed, using the spatial and temporal domains defined above. We download the physical field used to build the weather types (geopotential height anomalies at 500 mb in this case), winds (500 mb), rainfall and temperature fields. A 5-day moving average is applied to the fields before computing the daily anomalies (daily climatologies are computed via the na√Øve approach)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1104aff0-9d72-4b33-b7ab-9735219bec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "reanalysis = download_data(\n",
    "    #url='https://iridl.ldeo.columbia.edu/SOURCES/.ECMWF/.S2S/.ECMF/.reforecast/.perturbed/.pressure_level_gh/.gh/P/500/VALUE/P/removeGRID/S/(3%20Sep%202017)/VALUE/X/('+str(wlo)+')/('+str(elo)+')/RANGE/Y/('+str(sla)+')/('+str(nla)+')/RANGE/hdate/(1997)/(2016)/RANGE/hdate//pointwidth/0/def/-6/shiftGRID/hdate/(days%20since%201960-01-01)/streamgridunitconvert/S/(days%20since%202019-01-01)/streamgridunitconvert/S//units//days/def/L/0.5/add/hdate/add/add//pointwidth/1/def/SOURCES/.ECMWF/.ERA-Interim/.SIX-HOURLY/.pressure_level/.z500/T/3/shiftGRID/T/24/boxAverage/T/2/index/.units/streamgridunitconvert/exch/%5BT%5Dsample-along/c%3A/9.81/(m%20s-2)/%3Ac/div/S/removeGRID/L/5/runningAverage/%5BX/Y%5D/1.0/0./regridLinear/dup/%5Bhdate%5Daverage/2/RECHUNK/sub/data.nc',\n",
    "    #url='https://iridl.ldeo.columbia.edu/SOURCES/.NASA/.NCCS_Dataserver/.CREATE_REANALYSIS/.MERRA2/.day/.atmos/.zg/plev/500/VALUE/plev/removeGRID/X/('+str(wlo)+')/('+str(elo)+')/RANGE/Y/('+str(sla)+')/('+str(nla)+')/RANGE/T/-0.875/shiftGRID/T/('+season+'%20'+str(yeari)+'-'+str(yeare)+')/RANGE/data.nc',\n",
    "    #url='https://iridl.ldeo.columbia.edu/SOURCES/.NASA/.GSFC/.MERRA/.Anl_StdLev/.h500/T/1/boxAverage/X/('+str(wlo)+')/('+str(elo)+')/RANGE/Y/('+str(sla)+')/('+str(nla)+')/RANGE/T/5/runningAverage/T/('+season+'%20'+str(yeari)+'-'+str(yeare)+')/RANGE/dup/%5BT%5D/average/sub/data.nc',\n",
    "    #url='http://iridl.ldeo.columbia.edu/home/.agmunoz/.NNRP/.chi_200/T/%28days%20since%201960-01-01%2000:00:00%29/streamgridunitconvert/T/('+season+'%20'+str(yeari)+'-'+str(yeare)+')/RANGE/T//pointwidth/0/def/-0.5/shiftGRID/X/('+str(wlo)+')/('+str(elo)+')/RANGE/Y/('+str(sla)+')/('+str(nla)+')/RANGE/dup/T/to366daysample/%5BYR%5Daverage/T/sampleDOY/sub/T/5/runningAverage/T/0.5/shiftGRID/data.nc',\n",
    "    url='http://iridl.ldeo.columbia.edu/SOURCES/.NOAA/.NCEP-NCAR/.CDAS-1/.DAILY/.Intrinsic/.PressureLevel/.phi/P/(500)/VALUE/T/%28days%20since%201960-01-01%2000:00:00%29/streamgridunitconvert/T/('+season+'%20'+str(yeari)+'-'+str(yeare)+')/RANGE/T//pointwidth/0/def/-0.5/shiftGRID/X/('+str(wlo)+')/('+str(elo)+')/RANGE/Y/('+str(sla)+')/('+str(nla)+')/RANGE/dup/T/to366daysample/%5BYR%5Daverage/T/sampleDOY/sub/T/5/runningAverage/T/0.5/shiftGRID/data.nc',\n",
    "    outfile='DWR/data/hgt_NNRP_rean.nc', \n",
    "    authkey=authkey,\n",
    "    force_download=force\n",
    ").stack(time=['T'], grid=['Y', 'X'])\n",
    "\n",
    "reanalysis=reanalysis.isel(P=0)\n",
    "#reanalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b05cc1c-6210-4ee3-b4af-01c68686a477",
   "metadata": {},
   "outputs": [],
   "source": [
    "rainfall = download_data(\n",
    "    url='http://iridl.ldeo.columbia.edu/SOURCES/.NOAA/.NCEP/.CPC/.UNIFIED_PRCP/.GAUGE_BASED/.GLOBAL/.v1p0/.extREALTIME/.rain/T/%28days%20since%201960-01-01%2000:00:00%29/streamgridunitconvert/T/%28%201%20Jan%20'+str(yeari)+'%29/%2830%20Dec%20'+str(yeare)+'%29/RANGEEDGES/T//pointwidth/0/def/0./shiftGRID/X/('+str(wlo)+')/('+str(elo)+')/RANGE/Y/('+str(sla)+')/('+str(nla)+')/RANGE/dup/T/to366daysample/%5BYR%5Daverage/T/sampleDOY/sub/T/5/runningAverage/T/0.5/shiftGRID/data.nc',\n",
    "    #url='https://iridl.ldeo.columbia.edu/SOURCES/.NOAA/.NCEP/.CPC/.UNIFIED_PRCP/.GAUGE_BASED/.GLOBAL/.v1p0/.extREALTIME/.rain/T/%28days%20since%201960-01-01%2000:00:00%29/streamgridunitconvert/T/('+season+'%20'+str(yeari)+'-'+str(yeare)+')/RANGE/T//pointwidth/0/def/-0.5/shiftGRID/X/('+str(wlo)+')/('+str(elo)+')/RANGE/Y/('+str(sla)+')/('+str(nla)+')/RANGE/dup/T/to366daysample/%5BYR%5Daverage/T/sampleDOY/sub/T/5/runningAverage/T/0.5/shiftGRID/data.nc',\n",
    "    #url='https://iridl.ldeo.columbia.edu/SOURCES/.NOAA/.NCEP/.CPC/.UNIFIED_PRCP/.GAUGE_BASED/.GLOBAL/.v1p0/.extREALTIME/.rain/X/('+str(wlo)+')/('+str(elo)+')/RANGE/Y/('+str(sla)+')/('+str(nla)+')/RANGE/T/5/runningAverage/T/('+season+'%20'+str(yeari)+'-'+str(yeare)+')/RANGE/dup/T/to366daysample/%5BYR%5Daverage/T/sampleDOY/sub/T/-0.5/shiftGRID/data.nc',\n",
    "    outfile='DWR/data/rainfall_cpc.nc', \n",
    "    authkey=authkey,\n",
    "    force_download=force\n",
    ").stack(time=['T'], grid=['Y', 'X'])\n",
    "#rainfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f658092-f9b0-4ac1-8028-812ac0b1660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m  = download_data(\n",
    "    url='http://iridl.ldeo.columbia.edu/SOURCES/.NOAA/.NCEP/.CPC/.temperature/.daily/.tmax/SOURCES/.NOAA/.NCEP/.CPC/.temperature/.daily/.tmin/add/2/div/T/(days%20since%201960-01-01%2000%3A00%3A00)/streamgridunitconvert/T/%28%201%20Jan%20'+str(yeari)+'%29/%2830%20Dec%20'+str(yeare)+'%29/RANGE/T//pointwidth/0/def/-0.5/shiftGRID/X/('+str(wlo)+')/('+str(elo)+')/RANGE/Y/('+str(sla)+')/('+str(nla)+')/RANGE/dup/T/to366daysample/%5BYR%5Daverage/T/sampleDOY/sub/T/5/runningAverage/T/0.5/shiftGRID/data.nc',\n",
    "    #url='http://iridl.ldeo.columbia.edu/SOURCES/.NOAA/.NCEP/.CPC/.temperature/.daily/.tmax/SOURCES/.NOAA/.NCEP/.CPC/.temperature/.daily/.tmin/add/2/div/X/('+str(wlo)+')/('+str(elo)+')/RANGE/Y/('+str(sla)+')/('+str(nla)+')/RANGE/T/5/runningAverage/T/('+season+'%20'+str(yeari)+'-'+str(yeare)+')/RANGE/dup/T/to366daysample%5BYR%5Daverage/T/sampleDOY/sub/data.nc',\n",
    "    outfile='DWR/data/t2m_cpc.nc', \n",
    "    authkey=authkey,\n",
    "    force_download=force\n",
    ").stack(time=['T'], grid=['Y', 'X'])\n",
    "\n",
    "#t2m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6c0448-0efa-4ba5-88ae-b38a851549a4",
   "metadata": {},
   "source": [
    "# Dimension Reduction\n",
    "\n",
    "We need to choose a percentage of variance explained that we will require as an input to get number of EOFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639a77e3-28e9-417a-a3da-416cb544b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eof = get_number_eof(X=reanalysis['adif'].values, var_to_explain=0.9, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec24c27-0f7d-4dae-a143-b6df61dbb0e6",
   "metadata": {},
   "source": [
    "Now we project the data onto the leading EOFs to get the principal component time series.\n",
    "We will retain the PCA model for use later.\n",
    "The `reanalysis_pc` variable is now indexed [`time`, `EOF`]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60272ff0-685f-4ae3-a4bb-773a8a618f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model = PCA(n_components=n_eof).fit(reanalysis['adif'].values)\n",
    "reanalysis_pc = pca_model.transform(reanalysis['adif'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0511d8-5663-4d86-908a-77ea618c5276",
   "metadata": {},
   "source": [
    "# Reanalysis Weather Typing\n",
    "\n",
    "Now we perform the clustering.\n",
    "We will manually specify the number of clusters we want to create and the number of simulations we want to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948c2eb6-893e-4d66-b481-b4814e803379",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncluster = 4 # use this number of WTs\n",
    "n_sim = 50 # typically 25-50 -- try 25 for quick preliminary computation only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abeeb42-7588-4234-a982-82baabe23c45",
   "metadata": {},
   "source": [
    "Now we can use this to run the classifiability index on our centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eccf63-542b-4799-ba2d-135a652669cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids, wtypes = loop_kmeans(X=reanalysis_pc, n_cluster=ncluster, n_sim=n_sim)\n",
    "class_idx, best_part = get_classifiability_index(centroids)\n",
    "print('The classifiability index is {}'.format(class_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cb0920-f594-47c5-a604-885fc61bce26",
   "metadata": {},
   "source": [
    "Let's reorder the WTs such that the most frequent is WT1, and the less frequent is WT_last.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0733757b-8f18-47cd-90bb-e64998893ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_wt = wtypes[best_part, :]\n",
    "best_wt = pd.Series(resort_labels(best_wt), index=reanalysis[\"time\"]).to_xarray()\n",
    "best_wt.name = \"wtype\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603f1778-2b52-4238-a49b-8f583e791982",
   "metadata": {},
   "source": [
    "Now that we have identified a suitable partition, we can use it to keep only the corresponding centroid and set of weather type labels.\n",
    "To take advantage of the scikit-learn syntax, we then use these centroids to define a `KMeans` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cf4fa1-75ad-4775-ab0e-5a4c1432e5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fit = KMeans(n_clusters=ncluster, init=centroids[best_part, :, :], n_init=1, max_iter=1).fit(reanalysis_pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aee189-778d-4a67-bdef-40f1cbef6b85",
   "metadata": {},
   "source": [
    "Start with the reanalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542d6dea-5a5e-4add-abf0-07e4eb571cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reanalysis_composite = reanalysis.copy()\n",
    "model_clust = best_fit.fit_predict(reanalysis_pc) # get centroids\n",
    "weather_types = xr.DataArray(\n",
    "    #model_clust, \n",
    "    best_wt,\n",
    "    coords = {'time': reanalysis_composite['time']},\n",
    "    dims='time'\n",
    ")\n",
    "reanalysis_composite['WT'] = weather_types\n",
    "reanalysis_composite = reanalysis_composite.groupby('WT').mean(dim='time').unstack('grid')['adif']\n",
    "reanalysis_composite['M'] = 0\n",
    "\n",
    "wt_anomalies = [] # initialize empty list\n",
    "wt_anomalies.append(reanalysis_composite)\n",
    "\n",
    "wt_anomalies = xr.concat(wt_anomalies, dim='M') # join together\n",
    "#If the WTs start at 0, this line of code shifts them so they start at 1\n",
    "wt_anomalies['WT'] = wt_anomalies['WT'] # start from 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314e6b89-09bf-4db9-aa0f-61d8f824ac9b",
   "metadata": {},
   "source": [
    "Now we prepare a figure with rainfall and temperature composites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb977b-532a-40e4-a288-9e339fd50cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = np.meshgrid(reanalysis['adif'].X, reanalysis['adif'].Y)\n",
    "map_proj = ccrs.PlateCarree() #ccrs.Orthographic(-110, 10)\n",
    "data_proj = ccrs.PlateCarree()\n",
    "wt_unique = np.unique(wt_anomalies['WT'])\n",
    "figsize = (14, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa6bdc4-76f7-4f3c-8bda-898ae2b44f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WT proportions\n",
    "wt=weather_types.to_dataframe(name='WT')\n",
    "wt=wt+1\n",
    "wt_counts = wt.groupby('WT').size().div(wt['WT'].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f88259-ab75-4f09-bf92-48ff12e74e96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
